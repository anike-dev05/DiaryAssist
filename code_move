To convert the given SAS code to PySpark, you need to use the PySpark DataFrame API to handle date conversions and conditional assignments. Hereâ€™s how you can do it:

### Original SAS Code
```sas
%CONVERTDATE(R_AC_OPN_DT, AC_OPN_DATE);
IF NOT MISSING AC_OPN_DATE THEN ACCOUNT_APENED_DT = PUT(AC_OPN_DATE, YYMMDD10.);
```

### Equivalent PySpark Code
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, date_format, to_date

# Initialize Spark session
spark = SparkSession.builder.appName("Convert SAS to PySpark").getOrCreate()

# Sample DataFrame for illustration
# Assuming R_AC_OPN_DT and AC_OPN_DATE are columns in the DataFrame df
data = [
    ("2021-06-01", None),
    ("2023-07-15", "2023-07-15"),
    (None, "2022-05-20")
]
columns = ["R_AC_OPN_DT", "AC_OPN_DATE"]
df = spark.createDataFrame(data, columns)

# Convert R_AC_OPN_DT to AC_OPN_DATE (assuming the format is the same for simplicity)
# If the date formats differ, additional conversion logic may be needed
df = df.withColumn("AC_OPN_DATE", when(col("AC_OPN_DATE").isNull(), col("R_AC_OPN_DT")).otherwise(col("AC_OPN_DATE")))

# Apply the conditional assignment and format the date to 'yyyyMMdd'
df = df.withColumn(
    "ACCOUNT_APENED_DT",
    when(col("AC_OPN_DATE").isNotNull(), date_format(to_date(col("AC_OPN_DATE"), 'yyyy-MM-dd'), 'yyyyMMdd'))
)

# Show the resulting DataFrame
df.show()
```

### Explanation
1. **Initialize Spark Session**: Create a Spark session.
2. **Sample DataFrame**: For illustration purposes, a sample DataFrame is created. Replace it with your actual DataFrame.
3. **Convert Date**: The date conversion logic assumes that if `AC_OPN_DATE` is null, it should take the value from `R_AC_OPN_DT`.
4. **Conditional Assignment**: The `when` function is used to conditionally assign the value of `ACCOUNT_APENED_DT` if `AC_OPN_DATE` is not null. The `date_format` function formats the date in `yyyyMMdd` format.
5. **Show DataFrame**: Display the resulting DataFrame to verify the changes.