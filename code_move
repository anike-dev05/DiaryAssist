To convert the SAS expression `weighted_bad_flag = bad_ind * weight;` into PySpark, you can directly use DataFrame operations to compute the product of `bad_ind` and `weight` columns. Here's how you can implement it:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Assuming you have a SparkSession named 'spark'
spark = SparkSession.builder \
    .appName("Weighted Bad Flag Calculation") \
    .getOrCreate()

# Assuming you have a DataFrame named 'df' containing the relevant columns
# bad_ind, weight

df = spark.read.csv("path_to_your_data.csv", header=True)  # Adjust as per your data source

# Adding a new column 'weighted_bad_flag' based on the expression bad_ind * weight
df = df.withColumn("weighted_bad_flag", col("bad_ind") * col("weight"))

# Show the resulting DataFrame
df.show()
```

### Explanation:

1. **Reading Data**: Assumes you read your data into a DataFrame `df`.

2. **Column Operation**:
   - `.withColumn("weighted_bad_flag", col("bad_ind") * col("weight"))` computes the product of `bad_ind` and `weight` columns and adds a new column named "weighted_bad_flag" to the DataFrame `df`.

3. **Result**:
   - The `weighted_bad_flag` column in the resulting DataFrame will contain the computed values based on the multiplication of `bad_ind` and `weight`.

4. **Output**:
   - Finally, `.show()` displays the resulting DataFrame with the "weighted_bad_flag" column populated accordingly.

Ensure that your actual DataFrame (`df`) contains columns named `bad_ind` and `weight`, or adjust `col("bad_ind")` and `col("weight")` to match the actual column names in your DataFrame.

This conversion directly translates the SAS arithmetic operation into PySpark DataFrame operations, leveraging PySpark's capabilities to perform element-wise computations efficiently across large datasets. Adjustments may be necessary based on your specific data structure and requirements.