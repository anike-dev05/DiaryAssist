To convert the given SAS code to PySpark, we need to:

1. Check if the maximum value of a range of columns is missing (null).
2. Based on this check, assign a value to the `BAD_18` column.

Hereâ€™s how you can achieve this in PySpark:

1. **Create an example DataFrame** with the relevant columns.
2. **Use PySpark functions** to perform the missing value check and conditionally assign the value to `BAD_18`.

Here's the PySpark code:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, greatest, when, array

# Initialize Spark session
spark = SparkSession.builder.appName("SAS to PySpark").getOrCreate()

# Example DataFrame
data = [
    {"WODT0": 1, "WODT1": 2, "WODT18": 3, "DELQ0": 1, "DELQ1": 2, "DELQ18": 3},
    {"WODT0": None, "WODT1": None, "WODT18": None, "DELQ0": 1, "DELQ1": 3, "DELQ18": 2},
    {"WODT0": None, "WODT1": None, "WODT18": None, "DELQ0": None, "DELQ1": 1, "DELQ18": 2}
]

df = spark.createDataFrame(data)

# List of columns
wodt_columns = [f"WODT{i}" for i in range(19)]
delq_columns = [f"DELQ{i}" for i in range(19)]

# Check if the maximum of WODT columns is not null
max_wodt = greatest(*[col(c) for c in wodt_columns])

# Check if the maximum of DELQ columns is greater than or equal to 2
max_delq = greatest(*[col(c) for c in delq_columns])

# Apply the condition to create the BAD_18 column
df = df.withColumn(
    "BAD_18",
    when(max_wodt.isNotNull(), 1).otherwise(when(max_delq >= 2, 1).otherwise(0))
)

# Show the result
df.show(truncate=False)
```

### Explanation:

1. **Initialize Spark Session**: Creates a Spark session.
2. **Example DataFrame**: Constructs a sample DataFrame with the `WODT` and `DELQ` columns.
3. **List of Columns**: Create lists of the `WODT` and `DELQ` columns.
4. **Check if Maximum of WODT Columns is Not Null**: Use the `greatest` function to find the maximum value among the `WODT` columns and check if it is not null.
5. **Check if Maximum of DELQ Columns is Greater Than or Equal to 2**: Use the `greatest` function to find the maximum value among the `DELQ` columns and check if it is greater than or equal to 2.
6. **Apply the Condition to Create the BAD_18 Column**: Use the `when` function to apply the conditional logic and create the `BAD_18` column.
7. **Show the Result**: Display the resulting DataFrame.

### Output:

The output will be a DataFrame with the columns `WODT0`, `WODT1`, `WODT18`, `DELQ0`, `DELQ1`, `DELQ18`, and `BAD_18`:

```
+-----+-----+------+-----+-----+------+------+
|WODT0|WODT1|WODT18|DELQ0|DELQ1|DELQ18|BAD_18|
+-----+-----+------+-----+-----+------+------+
|1    |2    |3     |1    |2    |3     |1     |
|null |null |null  |1    |3    |2     |1     |
|null |null |null  |null |1    |2     |1     |
+-----+-----+------+-----+-----+------+------+
```

This PySpark code replicates the functionality of the SAS code by checking for missing values in the `WODT` columns and applying the conditional logic to create the `BAD_18` column.