from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

# Define UDFs for each utilization column
def util1_udf(bal, crlimit):
    return 0 if crlimit[0] in (0, None) else (bal[0] / crlimit[0]) * 100

def util2_udf(bal, crlimit):
    return 0 if crlimit[1] in (0, None) else (bal[1] / crlimit[1]) * 100

def util3_udf(bal, crlimit):
    return 0 if crlimit[2] in (0, None) else (bal[2] / crlimit[2]) * 100

# Register the UDFs
util1_udf = F.udf(util1_udf, DoubleType())
util2_udf = F.udf(util2_udf, DoubleType())
util3_udf = F.udf(util3_udf, DoubleType())

# Apply the UDFs to create the three columns in the DataFrame
df = df.withColumn('util1', util1_udf(F.col('bal'), F.col('crlimit'))) \
       .withColumn('util2', util2_udf(F.col('bal'), F.col('crlimit'))) \
       .withColumn('util3', util3_udf(F.col('bal'), F.col('crlimit')))