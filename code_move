from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when

# Assuming 'df' is your PySpark DataFrame

# Define conditions and corresponding exclusions
conditions = [
    (col("n_age_of_prim_appl") < 21) | (col("n_age_of_prim_appl") > 65, "1.AGE EXCL"),
    (col("p_dbt_brdn_ratio") > 50, "2.DBR EXCL"),
    (col("CC_HUB_PROD").isin('AME', 'HML'), "3.OTHER PRODUCTS EXCL"),
    (col("CC_MON_WITH_HSBC") < 6) & (~col("cc_package_code").isin('A', 'F', 'G', 'H', '', None), "4.NAEL CUSTOMERS LT 6 MTHS EXCL"),
    (col("br_wrst_del_112m") > 1, "5.BUREAU DELQ EXCL"),
    ((col("a_tot_anul_inc") / 12) < 7500, "6.INCOME RELATED EXCL")
]

# Applying the conditions
for condition, exclusion_label in conditions:
    df = df.withColumn("Exclusion", when(condition, exclusion_label))

# Default case if none of the conditions match
df = df.withColumn("Exclusion", when(col("Exclusion").isNull(), "8.Included").otherwise(col("Exclusion")))

# Show or further process the DataFrame
df.show()