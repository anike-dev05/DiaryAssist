To convert the given SAS code snippet into PySpark syntax, you'll need to translate the SAS data step logic into PySpark DataFrame operations. Here's how you can do it:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, to_date

# Assuming you have a SparkSession already created
spark = SparkSession.builder.appName("SAS to PySpark Conversion").getOrCreate()

# Example DataFrame with columns matching SAS variables
data = spark.createDataFrame([
    (12345, '2023-05-15'),
    (67890, '2024-01-20')
], ["MDC_RSTR_RELORIG_ACNO", "ACCOUNT_OPENED_DT"])

# Convert ACCOUNT_OPENED_DT to date format if not already done
data = data.withColumn("AC_OPN_DATE", to_date(col("ACCOUNT_OPENED_DT"), 'yyyy-MM-dd'))

# Define the conditions and apply transformations
data = data.withColumn("IFRS_RSTR_IND", 
                       when((col("MDC_RSTR_RELORIG_ACNO") == "RSTR_DT1"), 'Y').otherwise('N'))

data = data.withColumn("RSTR_DT1", to_date(col("RSTR_DT*1"), 'yyyyMMdd'))

data = data.withColumn("D_RSTR_DT", to_date(col("DTRSTR_DT1"), 'yyyyMMdd'))

data = data.withColumn("IFRS_RSTR_IND", 
                       when((col("IFRS_RSTR_IND") == 'Y') & (col("AC_OPN_DATE") > col("D_RSTR_DT")), 'N').otherwise(col("IFRS_RSTR_IND")))

# Show the resulting DataFrame
data.show()
```

### Explanation:

1. **DataFrame Creation**: `data` is created with example data and assumed columns (`MDC_RSTR_RELORIG_ACNO`, `ACCOUNT_OPENED_DT`).

2. **Date Conversion**: `to_date()` function is used to convert string dates to `DateType` in PySpark.

3. **Conditional Logic**:
   - `IFRS_RSTR_IND` is derived using `when()` and `otherwise()` functions, similar to SAS `IF-THEN-ELSE` logic.
   - Date comparisons (`AC_OPN_DATE > D_RSTR_DT`) are handled within `when()`.

4. **Column Renaming**: Ensure that column names are adjusted to match your actual data schema in PySpark.

5. **Output**: Finally, `data.show()` displays the transformed DataFrame.

Adjust column names (`RSTR_DT*1`, `DTRSTR_DT1`, etc.) and formats (`yyyyMMdd`, `yyyy-MM-dd`, etc.) based on your actual data and requirements. This conversion approach should mirror the logic and transformations you need from SAS to PySpark.